{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ef443936",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Byte-Pair Encoding (BPE) was initially developed as an algorithm to compress texts, and then used by OpenAI for tokenization when pretraining the GPT model. It‚Äôs used by a lot of Transformer models, including GPT, GPT-2, RoBERTa, BART, and DeBERTa.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "11aedc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "Byte-Pair Encoding (BPE) was initially developed as an algorithm to compress texts, and then used by OpenAI for tokenization when pretraining the GPT model. It‚Äôs used by a lot of Transformer models, including GPT, GPT-2, RoBERTa, BART, and DeBERTa.\n",
      "\n",
      "Length: 248\n",
      "\n",
      "Tokens:\n",
      "[66, 121, 116, 101, 45, 80, 97, 105, 114, 32, 69, 110, 99, 111, 100, 105, 110, 103, 32, 40, 66, 80, 69, 41, 32, 119, 97, 115, 32, 105, 110, 105, 116, 105, 97, 108, 108, 121, 32, 100, 101, 118, 101, 108, 111, 112, 101, 100, 32, 97, 115, 32, 97, 110, 32, 97, 108, 103, 111, 114, 105, 116, 104, 109, 32, 116, 111, 32, 99, 111, 109, 112, 114, 101, 115, 115, 32, 116, 101, 120, 116, 115, 44, 32, 97, 110, 100, 32, 116, 104, 101, 110, 32, 117, 115, 101, 100, 32, 98, 121, 32, 79, 112, 101, 110, 65, 73, 32, 102, 111, 114, 32, 116, 111, 107, 101, 110, 105, 122, 97, 116, 105, 111, 110, 32, 119, 104, 101, 110, 32, 112, 114, 101, 116, 114, 97, 105, 110, 105, 110, 103, 32, 116, 104, 101, 32, 71, 80, 84, 32, 109, 111, 100, 101, 108, 46, 32, 73, 116, 226, 128, 153, 115, 32, 117, 115, 101, 100, 32, 98, 121, 32, 97, 32, 108, 111, 116, 32, 111, 102, 32, 84, 114, 97, 110, 115, 102, 111, 114, 109, 101, 114, 32, 109, 111, 100, 101, 108, 115, 44, 32, 105, 110, 99, 108, 117, 100, 105, 110, 103, 32, 71, 80, 84, 44, 32, 71, 80, 84, 45, 50, 44, 32, 82, 111, 66, 69, 82, 84, 97, 44, 32, 66, 65, 82, 84, 44, 32, 97, 110, 100, 32, 68, 101, 66, 69, 82, 84, 97, 46]\n",
      "\n",
      "Token data type: <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "tokens = text.encode('utf-8')\n",
    "tokens = list(tokens)\n",
    "\n",
    "print(f\"Text:\\n{text}\\n\")\n",
    "print(f\"Length: {len(text)}\\n\")\n",
    "print(f\"Tokens:\\n{tokens}\\n\")\n",
    "print(f\"Token data type: {type(tokens[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f951087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_frequency(tokens):\n",
    "    hashmap = {}\n",
    "    \n",
    "    for index in range(len(tokens) - 1):\n",
    "        x, y = tokens[index], tokens[index + 1]\n",
    "        hashmap[(x,y)] = hashmap.get((x,y), 0) + 1\n",
    "    \n",
    "    return hashmap        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "daded364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair frequency:\n",
      "{(66, 121): 1, (121, 116): 1, (116, 101): 2, (101, 45): 1, (45, 80): 1, (80, 97): 1, (97, 105): 2, (105, 114): 1, (114, 32): 3, (32, 69): 1, (69, 110): 1, (110, 99): 2, (99, 111): 2, (111, 100): 3, (100, 105): 2, (105, 110): 6, (110, 103): 3, (103, 32): 3, (32, 40): 1, (40, 66): 1, (66, 80): 1, (80, 69): 1, (69, 41): 1, (41, 32): 1, (32, 119): 2, (119, 97): 1, (97, 115): 2, (115, 32): 4, (32, 105): 2, (110, 105): 3, (105, 116): 2, (116, 105): 2, (105, 97): 1, (97, 108): 2, (108, 108): 1, (108, 121): 1, (121, 32): 3, (32, 100): 1, (100, 101): 3, (101, 118): 1, (118, 101): 1, (101, 108): 3, (108, 111): 2, (111, 112): 1, (112, 101): 2, (101, 100): 3, (100, 32): 5, (32, 97): 6, (97, 110): 4, (110, 32): 4, (108, 103): 1, (103, 111): 1, (111, 114): 3, (114, 105): 1, (116, 104): 3, (104, 109): 1, (109, 32): 1, (32, 116): 5, (116, 111): 2, (111, 32): 1, (32, 99): 1, (111, 109): 1, (109, 112): 1, (112, 114): 2, (114, 101): 2, (101, 115): 1, (115, 115): 1, (101, 120): 1, (120, 116): 1, (116, 115): 1, (115, 44): 2, (44, 32): 6, (110, 100): 2, (104, 101): 3, (101, 110): 4, (32, 117): 2, (117, 115): 2, (115, 101): 2, (32, 98): 2, (98, 121): 2, (32, 79): 1, (79, 112): 1, (110, 65): 1, (65, 73): 1, (73, 32): 1, (32, 102): 1, (102, 111): 2, (111, 107): 1, (107, 101): 1, (105, 122): 1, (122, 97): 1, (97, 116): 1, (105, 111): 1, (111, 110): 1, (119, 104): 1, (32, 112): 1, (101, 116): 1, (116, 114): 1, (114, 97): 2, (101, 32): 1, (32, 71): 3, (71, 80): 3, (80, 84): 3, (84, 32): 1, (32, 109): 2, (109, 111): 2, (108, 46): 1, (46, 32): 1, (32, 73): 1, (73, 116): 1, (116, 226): 1, (226, 128): 1, (128, 153): 1, (153, 115): 1, (97, 32): 1, (32, 108): 1, (111, 116): 1, (116, 32): 1, (32, 111): 1, (111, 102): 1, (102, 32): 1, (32, 84): 1, (84, 114): 1, (110, 115): 1, (115, 102): 1, (114, 109): 1, (109, 101): 1, (101, 114): 1, (108, 115): 1, (99, 108): 1, (108, 117): 1, (117, 100): 1, (84, 44): 2, (84, 45): 1, (45, 50): 1, (50, 44): 1, (32, 82): 1, (82, 111): 1, (111, 66): 1, (66, 69): 2, (69, 82): 2, (82, 84): 3, (84, 97): 2, (97, 44): 1, (32, 66): 1, (66, 65): 1, (65, 82): 1, (32, 68): 1, (68, 101): 1, (101, 66): 1, (97, 46): 1}\n"
     ]
    }
   ],
   "source": [
    "hashmap = get_pair_frequency(tokens)\n",
    "\n",
    "print(f\"Pair frequency:\\n{hashmap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a1c0411d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reverse mapping:\n",
      "{(1, (116, 226)), (2, (97, 105)), (1, (32, 108)), (1, (120, 116)), (1, (97, 32)), (2, (100, 105)), (4, (97, 110)), (3, (116, 104)), (2, (115, 44)), (3, (110, 105)), (1, (84, 114)), (1, (116, 115)), (1, (110, 115)), (2, (112, 101)), (3, (82, 84)), (3, (101, 100)), (2, (117, 115)), (1, (66, 80)), (1, (32, 40)), (2, (32, 117)), (1, (32, 100)), (6, (32, 97)), (1, (109, 101)), (1, (101, 66)), (2, (105, 116)), (1, (115, 115)), (2, (102, 111)), (1, (114, 109)), (1, (101, 118)), (4, (110, 32)), (1, (66, 65)), (2, (110, 99)), (1, (122, 97)), (2, (97, 108)), (1, (32, 66)), (1, (116, 114)), (1, (121, 116)), (1, (117, 100)), (5, (32, 116)), (1, (105, 122)), (3, (111, 100)), (4, (115, 32)), (2, (116, 101)), (1, (32, 73)), (1, (105, 97)), (2, (84, 44)), (3, (110, 103)), (3, (80, 84)), (1, (111, 66)), (1, (65, 82)), (5, (100, 32)), (2, (116, 111)), (1, (32, 84)), (1, (108, 46)), (2, (109, 111)), (1, (111, 32)), (1, (101, 116)), (1, (82, 111)), (1, (73, 116)), (3, (121, 32)), (2, (84, 97)), (3, (104, 101)), (6, (105, 110)), (1, (108, 108)), (2, (108, 111)), (2, (114, 101)), (2, (32, 119)), (1, (108, 115)), (1, (80, 69)), (1, (109, 112)), (1, (114, 105)), (1, (111, 109)), (2, (110, 100)), (2, (112, 114)), (2, (66, 69)), (1, (101, 32)), (4, (101, 110)), (1, (105, 111)), (3, (32, 71)), (1, (41, 32)), (1, (111, 116)), (3, (71, 80)), (2, (32, 105)), (1, (32, 68)), (1, (119, 104)), (1, (111, 112)), (1, (226, 128)), (1, (84, 32)), (1, (101, 115)), (1, (108, 103)), (1, (32, 79)), (1, (65, 73)), (1, (69, 110)), (1, (80, 97)), (1, (99, 108)), (2, (99, 111)), (1, (32, 111)), (1, (79, 112)), (1, (66, 121)), (1, (73, 32)), (1, (103, 111)), (2, (116, 105)), (1, (97, 46)), (1, (101, 114)), (1, (32, 112)), (1, (111, 110)), (1, (69, 41)), (3, (111, 114)), (1, (118, 101)), (1, (104, 109)), (1, (32, 99)), (2, (98, 121)), (1, (108, 121)), (3, (103, 32)), (1, (32, 69)), (1, (101, 120)), (1, (40, 66)), (2, (97, 115)), (1, (111, 107)), (1, (102, 32)), (1, (108, 117)), (1, (45, 80)), (1, (111, 102)), (1, (153, 115)), (1, (110, 65)), (1, (101, 45)), (2, (69, 82)), (1, (46, 32)), (1, (45, 50)), (1, (107, 101)), (1, (50, 44)), (1, (97, 44)), (2, (115, 101)), (2, (32, 98)), (1, (119, 97)), (1, (105, 114)), (1, (97, 116)), (3, (114, 32)), (1, (128, 153)), (1, (32, 102)), (1, (32, 82)), (1, (84, 45)), (3, (100, 101)), (2, (32, 109)), (2, (114, 97)), (1, (116, 32)), (1, (68, 101)), (3, (101, 108)), (6, (44, 32)), (1, (109, 32)), (1, (115, 102))}\n"
     ]
    }
   ],
   "source": [
    "reverse_mapping = {(value, key) for key, value in hashmap.items()}\n",
    "\n",
    "print(f\"Reverse mapping:\\n{reverse_mapping}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0448ceaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, (105, 110)), (6, (44, 32)), (6, (32, 97)), (5, (100, 32)), (5, (32, 116)), (4, (115, 32)), (4, (110, 32)), (4, (101, 110)), (4, (97, 110)), (3, (121, 32)), (3, (116, 104)), (3, (114, 32)), (3, (111, 114)), (3, (111, 100)), (3, (110, 105)), (3, (110, 103)), (3, (104, 101)), (3, (103, 32)), (3, (101, 108)), (3, (101, 100)), (3, (100, 101)), (3, (82, 84)), (3, (80, 84)), (3, (71, 80)), (3, (32, 71)), (2, (117, 115)), (2, (116, 111)), (2, (116, 105)), (2, (116, 101)), (2, (115, 101)), (2, (115, 44)), (2, (114, 101)), (2, (114, 97)), (2, (112, 114)), (2, (112, 101)), (2, (110, 100)), (2, (110, 99)), (2, (109, 111)), (2, (108, 111)), (2, (105, 116)), (2, (102, 111)), (2, (100, 105)), (2, (99, 111)), (2, (98, 121)), (2, (97, 115)), (2, (97, 108)), (2, (97, 105)), (2, (84, 97)), (2, (84, 44)), (2, (69, 82)), (2, (66, 69)), (2, (32, 119)), (2, (32, 117)), (2, (32, 109)), (2, (32, 105)), (2, (32, 98)), (1, (226, 128)), (1, (153, 115)), (1, (128, 153)), (1, (122, 97)), (1, (121, 116)), (1, (120, 116)), (1, (119, 104)), (1, (119, 97)), (1, (118, 101)), (1, (117, 100)), (1, (116, 226)), (1, (116, 115)), (1, (116, 114)), (1, (116, 32)), (1, (115, 115)), (1, (115, 102)), (1, (114, 109)), (1, (114, 105)), (1, (111, 116)), (1, (111, 112)), (1, (111, 110)), (1, (111, 109)), (1, (111, 107)), (1, (111, 102)), (1, (111, 66)), (1, (111, 32)), (1, (110, 115)), (1, (110, 65)), (1, (109, 112)), (1, (109, 101)), (1, (109, 32)), (1, (108, 121)), (1, (108, 117)), (1, (108, 115)), (1, (108, 108)), (1, (108, 103)), (1, (108, 46)), (1, (107, 101)), (1, (105, 122)), (1, (105, 114)), (1, (105, 111)), (1, (105, 97)), (1, (104, 109)), (1, (103, 111)), (1, (102, 32)), (1, (101, 120)), (1, (101, 118)), (1, (101, 116)), (1, (101, 115)), (1, (101, 114)), (1, (101, 66)), (1, (101, 45)), (1, (101, 32)), (1, (99, 108)), (1, (97, 116)), (1, (97, 46)), (1, (97, 44)), (1, (97, 32)), (1, (84, 114)), (1, (84, 45)), (1, (84, 32)), (1, (82, 111)), (1, (80, 97)), (1, (80, 69)), (1, (79, 112)), (1, (73, 116)), (1, (73, 32)), (1, (69, 110)), (1, (69, 41)), (1, (68, 101)), (1, (66, 121)), (1, (66, 80)), (1, (66, 65)), (1, (65, 82)), (1, (65, 73)), (1, (50, 44)), (1, (46, 32)), (1, (45, 80)), (1, (45, 50)), (1, (41, 32)), (1, (40, 66)), (1, (32, 112)), (1, (32, 111)), (1, (32, 108)), (1, (32, 102)), (1, (32, 100)), (1, (32, 99)), (1, (32, 84)), (1, (32, 82)), (1, (32, 79)), (1, (32, 73)), (1, (32, 69)), (1, (32, 68)), (1, (32, 66)), (1, (32, 40))]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(reverse_mapping, reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c798fd6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = sorted(reverse_mapping, reverse=True)\n",
    "type(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bb3e49bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = hashmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0a1efef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105, 110)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pair = max(mapping, key=mapping.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "61f806a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mint(tokens, pair, index):\n",
    "    new_tokens = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(tokens):\n",
    "        if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n",
    "            # This matches the token we want to replace\n",
    "            # Insert new token\n",
    "            new_tokens.append(index)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "    \n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dad986ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 24, 9, 1]\n"
     ]
    }
   ],
   "source": [
    "print(mint([5,6,6,7,9,1], (6,7), 24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0bd7dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Hugging Face's logo\n",
    "Hugging Face\n",
    "\n",
    "LLM Course documentation\n",
    "\n",
    "Byte-Pair Encoding tokenization\n",
    "\n",
    "\n",
    "\n",
    "Copy page\n",
    "\n",
    "Byte-Pair Encoding tokenization\n",
    "Ask a Question\n",
    "Open In Colab\n",
    "Open In Studio Lab\n",
    "Byte-Pair Encoding (BPE) was initially developed as an algorithm to compress texts, and then used by OpenAI for tokenization when pretraining the GPT model. It‚Äôs used by a lot of Transformer models, including GPT, GPT-2, RoBERTa, BART, and DeBERTa.\n",
    "\n",
    "\n",
    "üí° This section covers BPE in depth, going as far as showing a full implementation. You can skip to the end if you just want a general overview of the tokenization algorithm.\n",
    "\n",
    "Training algorithm\n",
    "BPE training starts by computing the unique set of words used in the corpus (after the normalization and pre-tokenization steps are completed), then building the vocabulary by taking all the symbols used to write those words. As a very simple example, let‚Äôs say our corpus uses these five words:\n",
    "\n",
    "Copied\n",
    "\"hug\", \"pug\", \"pun\", \"bun\", \"hugs\"\n",
    "The base vocabulary will then be [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"]. For real-world cases, that base vocabulary will contain all the ASCII characters, at the very least, and probably some Unicode characters as well. If an example you are tokenizing uses a character that is not in the training corpus, that character will be converted to the unknown token. That‚Äôs one reason why lots of NLP models are very bad at analyzing content with emojis, for instance.\n",
    "\n",
    "The GPT-2 and RoBERTa tokenizers (which are pretty similar) have a clever way to deal with this: they don‚Äôt look at words as being written with Unicode characters, but with bytes. This way the base vocabulary has a small size (256), but every character you can think of will still be included and not end up being converted to the unknown token. This trick is called byte-level BPE.\n",
    "\n",
    "After getting this base vocabulary, we add new tokens until the desired vocabulary size is reached by learning merges, which are rules to merge two elements of the existing vocabulary together into a new one. So, at the beginning these merges will create tokens with two characters, and then, as training progresses, longer subwords.\n",
    "\n",
    "At any step during the tokenizer training, the BPE algorithm will search for the most frequent pair of existing tokens (by ‚Äúpair,‚Äù here we mean two consecutive tokens in a word). That most frequent pair is the one that will be merged, and we rinse and repeat for the next step.\n",
    "\n",
    "Going back to our previous example, let‚Äôs assume the words had the following frequencies:\n",
    "\n",
    "Copied\n",
    "(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n",
    "meaning \"hug\" was present 10 times in the corpus, \"pug\" 5 times, \"pun\" 12 times, \"bun\" 4 times, and \"hugs\" 5 times. We start the training by splitting each word into characters (the ones that form our initial vocabulary) so we can see each word as a list of tokens:\n",
    "\n",
    "Copied\n",
    "(\"h\" \"u\" \"g\", 10), (\"p\" \"u\" \"g\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"u\" \"g\" \"s\", 5)\n",
    "Then we look at pairs. The pair (\"h\", \"u\") is present in the words \"hug\" and \"hugs\", so 15 times total in the corpus. It‚Äôs not the most frequent pair, though: that honor belongs to (\"u\", \"g\"), which is present in \"hug\", \"pug\", and \"hugs\", for a grand total of 20 times in the vocabulary.\n",
    "\n",
    "Thus, the first merge rule learned by the tokenizer is (\"u\", \"g\") -> \"ug\", which means that \"ug\" will be added to the vocabulary, and the pair should be merged in all the words of the corpus. At the end of this stage, the vocabulary and corpus look like this:\n",
    "\n",
    "Copied\n",
    "Vocabulary: [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\"]\n",
    "Corpus: (\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"ug\" \"s\", 5)\n",
    "Now we have some pairs that result in a token longer than two characters: the pair (\"h\", \"ug\"), for instance (present 15 times in the corpus). The most frequent pair at this stage is (\"u\", \"n\"), however, present 16 times in the corpus, so the second merge rule learned is (\"u\", \"n\") -> \"un\". Adding that to the vocabulary and merging all existing occurrences leads us to:\n",
    "\n",
    "Copied\n",
    "Vocabulary: [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\"]\n",
    "Corpus: (\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"h\" \"ug\" \"s\", 5)\n",
    "Now the most frequent pair is (\"h\", \"ug\"), so we learn the merge rule (\"h\", \"ug\") -> \"hug\", which gives us our first three-letter token. After the merge, the corpus looks like this:\n",
    "\n",
    "Copied\n",
    "Vocabulary: [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"]\n",
    "Corpus: (\"hug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"hug\" \"s\", 5)\n",
    "And we continue like this until we reach the desired vocabulary size.\n",
    "\n",
    "‚úèÔ∏è Now your turn! What do you think the next merge rule will be?\n",
    "\n",
    "Tokenization algorithm\n",
    "Tokenization follows the training process closely, in the sense that new inputs are tokenized by applying the following steps:\n",
    "\n",
    "Normalization\n",
    "Pre-tokenization\n",
    "Splitting the words into individual characters\n",
    "Applying the merge rules learned in order on those splits\n",
    "Let‚Äôs take the example we used during training, with the three merge rules learned:\n",
    "\n",
    "Copied\n",
    "(\"u\", \"g\") -> \"ug\"\n",
    "(\"u\", \"n\") -> \"un\"\n",
    "(\"h\", \"ug\") -> \"hug\"\n",
    "The word \"bug\" will be tokenized as [\"b\", \"ug\"]. \"mug\", however, will be tokenized as [\"[UNK]\", \"ug\"] since the letter \"m\" was not in the base vocabulary. Likewise, the word \"thug\" will be tokenized as [\"[UNK]\", \"hug\"]: the letter \"t\" is not in the base vocabulary, and applying the merge rules results first in \"u\" and \"g\" being merged and then \"h\" and \"ug\" being merged.\n",
    "\n",
    "‚úèÔ∏è Now your turn! How do you think the word \"unhug\" will be tokenized?\n",
    "\n",
    "Implementing BPE\n",
    "Now let‚Äôs take a look at an implementation of the BPE algorithm. This won‚Äôt be an optimized version you can actually use on a big corpus; we just want to show you the code so you can understand the algorithm a little bit better.\n",
    "\n",
    "First we need a corpus, so let‚Äôs create a simple one with a few sentences:\n",
    "\n",
    "Copied\n",
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]\n",
    "Next, we need to pre-tokenize that corpus into words. Since we are replicating a BPE tokenizer (like GPT-2), we will use the gpt2 tokenizer for the pre-tokenization:\n",
    "\n",
    "Copied\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "Then we compute the frequencies of each word in the corpus as we do the pre-tokenization:\n",
    "\n",
    "Copied\n",
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "print(word_freqs)\n",
    "Copied\n",
    "defaultdict(int, {'This': 3, 'ƒ†is': 2, 'ƒ†the': 1, 'ƒ†Hugging': 1, 'ƒ†Face': 1, 'ƒ†Course': 1, '.': 4, 'ƒ†chapter': 1,\n",
    "    'ƒ†about': 1, 'ƒ†tokenization': 1, 'ƒ†section': 1, 'ƒ†shows': 1, 'ƒ†several': 1, 'ƒ†tokenizer': 1, 'ƒ†algorithms': 1,\n",
    "    'Hopefully': 1, ',': 1, 'ƒ†you': 1, 'ƒ†will': 1, 'ƒ†be': 1, 'ƒ†able': 1, 'ƒ†to': 1, 'ƒ†understand': 1, 'ƒ†how': 1,\n",
    "    'ƒ†they': 1, 'ƒ†are': 1, 'ƒ†trained': 1, 'ƒ†and': 1, 'ƒ†generate': 1, 'ƒ†tokens': 1})\n",
    "The next step is to compute the base vocabulary, formed by all the characters used in the corpus:\n",
    "\n",
    "Copied\n",
    "alphabet = []\n",
    "\n",
    "for word in word_freqs.keys():\n",
    "    for letter in word:\n",
    "        if letter not in alphabet:\n",
    "            alphabet.append(letter)\n",
    "alphabet.sort()\n",
    "\n",
    "print(alphabet)\n",
    "Copied\n",
    "[ ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's',\n",
    "  't', 'u', 'v', 'w', 'y', 'z', 'ƒ†']\n",
    "We also add the special tokens used by the model at the beginning of that vocabulary. In the case of GPT-2, the only special token is \"<|endoftext|>\":\n",
    "\n",
    "Copied\n",
    "vocab = [\"<|endoftext|>\"] + alphabet.copy()\n",
    "We now need to split each word into individual characters, to be able to start training:\n",
    "\n",
    "Copied\n",
    "splits = {word: [c for c in word] for word in word_freqs.keys()}\n",
    "Now that we are ready for training, let‚Äôs write a function that computes the frequency of each pair. We‚Äôll need to use this at each step of the training:\n",
    "\n",
    "Copied\n",
    "def compute_pair_freqs(splits):\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            pair_freqs[pair] += freq\n",
    "    return pair_freqs\n",
    "Let‚Äôs have a look at a part of this dictionary after the initial splits:\n",
    "\n",
    "Copied\n",
    "pair_freqs = compute_pair_freqs(splits)\n",
    "\n",
    "for i, key in enumerate(pair_freqs.keys()):\n",
    "    print(f\"{key}: {pair_freqs[key]}\")\n",
    "    if i >= 5:\n",
    "        break\n",
    "Copied\n",
    "('T', 'h'): 3\n",
    "('h', 'i'): 3\n",
    "('i', 's'): 5\n",
    "('ƒ†', 'i'): 2\n",
    "('ƒ†', 't'): 7\n",
    "('t', 'h'): 3\n",
    "Now, finding the most frequent pair only takes a quick loop:\n",
    "\n",
    "Copied\n",
    "best_pair = \"\"\n",
    "max_freq = None\n",
    "\n",
    "for pair, freq in pair_freqs.items():\n",
    "    if max_freq is None or max_freq < freq:\n",
    "        best_pair = pair\n",
    "        max_freq = freq\n",
    "\n",
    "print(best_pair, max_freq)\n",
    "Copied\n",
    "('ƒ†', 't') 7\n",
    "So the first merge to learn is ('ƒ†', 't') -> 'ƒ†t', and we add 'ƒ†t' to the vocabulary:\n",
    "\n",
    "Copied\n",
    "merges = {(\"ƒ†\", \"t\"): \"ƒ†t\"}\n",
    "vocab.append(\"ƒ†t\")\n",
    "To continue, we need to apply that merge in our splits dictionary. Let‚Äôs write another function for this:\n",
    "\n",
    "Copied\n",
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                split = split[:i] + [a + b] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits\n",
    "And we can have a look at the result of the first merge:\n",
    "\n",
    "Copied\n",
    "splits = merge_pair(\"ƒ†\", \"t\", splits)\n",
    "print(splits[\"ƒ†trained\"])\n",
    "Copied\n",
    "['ƒ†t', 'r', 'a', 'i', 'n', 'e', 'd']\n",
    "Now we have everything we need to loop until we have learned all the merges we want. Let‚Äôs aim for a vocab size of 50:\n",
    "\n",
    "Copied\n",
    "vocab_size = 50\n",
    "\n",
    "while len(vocab) < vocab_size:\n",
    "    pair_freqs = compute_pair_freqs(splits)\n",
    "    best_pair = \"\"\n",
    "    max_freq = None\n",
    "    for pair, freq in pair_freqs.items():\n",
    "        if max_freq is None or max_freq < freq:\n",
    "            best_pair = pair\n",
    "            max_freq = freq\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "    vocab.append(best_pair[0] + best_pair[1])\n",
    "As a result, we‚Äôve learned 19 merge rules (the initial vocabulary had a size of 31 ‚Äî 30 characters in the alphabet, plus the special token):\n",
    "\n",
    "Copied\n",
    "print(merges)\n",
    "Copied\n",
    "{('ƒ†', 't'): 'ƒ†t', ('i', 's'): 'is', ('e', 'r'): 'er', ('ƒ†', 'a'): 'ƒ†a', ('ƒ†t', 'o'): 'ƒ†to', ('e', 'n'): 'en',\n",
    " ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('ƒ†to', 'k'): 'ƒ†tok',\n",
    " ('ƒ†tok', 'en'): 'ƒ†token', ('n', 'd'): 'nd', ('ƒ†', 'is'): 'ƒ†is', ('ƒ†t', 'h'): 'ƒ†th', ('ƒ†th', 'e'): 'ƒ†the',\n",
    " ('i', 'n'): 'in', ('ƒ†a', 'b'): 'ƒ†ab', ('ƒ†token', 'i'): 'ƒ†tokeni'}\n",
    "And the vocabulary is composed of the special token, the initial alphabet, and all the results of the merges:\n",
    "\n",
    "Copied\n",
    "print(vocab)\n",
    "Copied\n",
    "['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o',\n",
    " 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'ƒ†', 'ƒ†t', 'is', 'er', 'ƒ†a', 'ƒ†to', 'en', 'Th', 'This', 'ou', 'se',\n",
    " 'ƒ†tok', 'ƒ†token', 'nd', 'ƒ†is', 'ƒ†th', 'ƒ†the', 'in', 'ƒ†ab', 'ƒ†tokeni']\n",
    "üí° Using train_new_from_iterator() on the same corpus won‚Äôt result in the exact same vocabulary. This is because when there is a choice of the most frequent pair, we selected the first one encountered, while the ü§ó Tokenizers library selects the first one based on its inner IDs.\n",
    "\n",
    "To tokenize a new text, we pre-tokenize it, split it, then apply all the merge rules learned:\n",
    "\n",
    "Copied\n",
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
    "    for pair, merge in merges.items():\n",
    "        for idx, split in enumerate(splits):\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[idx] = split\n",
    "\n",
    "    return sum(splits, [])\n",
    "We can try this on any text composed of characters in the alphabet:\n",
    "\n",
    "Copied\n",
    "tokenize(\"This is not a token.\")\n",
    "Copied\n",
    "['This', 'ƒ†is', 'ƒ†', 'n', 'o', 't', 'ƒ†a', 'ƒ†token', '.']\n",
    "‚ö†Ô∏è Our implementation will throw an error if there is an unknown character since we didn‚Äôt do anything to handle them. GPT-2 doesn‚Äôt actually have an unknown token (it‚Äôs impossible to get an unknown character when using byte-level BPE), but this could happen here because we did not include all the possible bytes in the initial vocabulary. This aspect of BPE is beyond the scope of this section, so we‚Äôve left the details out.\n",
    "\n",
    "That‚Äôs it for the BPE algorithm! Next, we‚Äôll have a look at WordPiece.\n",
    "\n",
    "Update on GitHub\n",
    "‚Üê\n",
    "Normalization and pre-tokenization\n",
    "WordPiece tokenization\n",
    "‚Üí\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "429ca38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = text.encode(\"utf-8\")\n",
    "tokens = list(map(int, tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b2f71df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13399"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9f130588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13399"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = list(tokens)\n",
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6a6a3cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging (32, 32) into new token 256\n",
      "Merging (44, 32) into new token 257\n",
      "Merging (101, 32) into new token 258\n",
      "Merging (32, 116) into new token 259\n",
      "Merging (256, 256) into new token 260\n",
      "Merging (105, 110) into new token 261\n",
      "Merging (115, 32) into new token 262\n",
      "Merging (257, 39) into new token 263\n",
      "Merging (101, 110) into new token 264\n",
      "Merging (259, 104) into new token 265\n",
      "Merging (111, 114) into new token 266\n",
      "Merging (101, 114) into new token 267\n",
      "Merging (39, 263) into new token 268\n",
      "Merging (116, 32) into new token 269\n",
      "Merging (114, 101) into new token 270\n",
      "Merging (265, 258) into new token 271\n",
      "Merging (34, 257) into new token 272\n",
      "Merging (111, 107) into new token 273\n",
      "Merging (101, 100) into new token 274\n",
      "Merging (105, 116) into new token 275\n",
      "Merging (32, 97) into new token 276\n",
      "Merging (196, 160) into new token 277\n",
      "Merging (273, 264) into new token 278\n",
      "Merging (111, 110) into new token 279\n",
      "Merging (97, 114) into new token 280\n",
      "Merging (112, 108) into new token 281\n",
      "Merging (58, 32) into new token 282\n",
      "Merging (10, 260) into new token 283\n",
      "Merging (105, 114) into new token 284\n",
      "Merging (32, 119) into new token 285\n",
      "Merging (105, 122) into new token 286\n",
      "Merging (261, 103) into new token 287\n",
      "Merging (10, 10) into new token 288\n",
      "Merging (97, 284) into new token 289\n",
      "Merging (97, 98) into new token 290\n",
      "Merging (117, 108) into new token 291\n",
      "Merging (32, 34) into new token 292\n",
      "Merging (272, 34) into new token 293\n",
      "Merging (266, 100) into new token 294\n",
      "Merging (117, 103) into new token 295\n",
      "Merging (112, 289) into new token 296\n",
      "Merging (278, 286) into new token 297\n",
      "Merging (32, 115) into new token 298\n",
      "Merging (97, 110) into new token 299\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 300\n",
    "num_merges = vocab_size - 256\n",
    "\n",
    "merges = {}\n",
    "\n",
    "for i in range(num_merges):\n",
    "    freq = get_pair_frequency(temp)\n",
    "    top_pair = max(freq, key=freq.get)\n",
    "    index = 256 + i\n",
    "    print(f\"Merging {top_pair} into new token {index}\")\n",
    "    temp = mint(temp, top_pair, index)\n",
    "    merges[top_pair] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7855571f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9089"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1827264e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before compression: 13399\n",
      "After compression: 9089\n",
      "Compression ratio: 1.47x\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before compression: {len(tokens)}\")\n",
    "print(f\"After compression: {len(temp)}\")\n",
    "print(f\"Compression ratio: {len(tokens)/len(temp):.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "64e09088",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {index:bytes([index]) for index in range(256)}\n",
    "\n",
    "for (x,y), index in merges.items():\n",
    "    vocab[index] = vocab[x] + vocab[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "836fc2b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "eb5eb478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids):\n",
    "    \"\"\"\n",
    "    Input: ids (list of integers)\n",
    "    Output: string\n",
    "    \"\"\"\n",
    "    \n",
    "    # concatenate the bytes\n",
    "    tokens = b\"\".join(vocab[index] for index in ids)\n",
    "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "db292c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'an'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode([299])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4089f88a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(32, 32): 256,\n",
       " (44, 32): 257,\n",
       " (101, 32): 258,\n",
       " (32, 116): 259,\n",
       " (256, 256): 260,\n",
       " (105, 110): 261,\n",
       " (115, 32): 262,\n",
       " (257, 39): 263,\n",
       " (101, 110): 264,\n",
       " (259, 104): 265,\n",
       " (111, 114): 266,\n",
       " (101, 114): 267,\n",
       " (39, 263): 268,\n",
       " (116, 32): 269,\n",
       " (114, 101): 270,\n",
       " (265, 258): 271,\n",
       " (34, 257): 272,\n",
       " (111, 107): 273,\n",
       " (101, 100): 274,\n",
       " (105, 116): 275,\n",
       " (32, 97): 276,\n",
       " (196, 160): 277,\n",
       " (273, 264): 278,\n",
       " (111, 110): 279,\n",
       " (97, 114): 280,\n",
       " (112, 108): 281,\n",
       " (58, 32): 282,\n",
       " (10, 260): 283,\n",
       " (105, 114): 284,\n",
       " (32, 119): 285,\n",
       " (105, 122): 286,\n",
       " (261, 103): 287,\n",
       " (10, 10): 288,\n",
       " (97, 284): 289,\n",
       " (97, 98): 290,\n",
       " (117, 108): 291,\n",
       " (32, 34): 292,\n",
       " (272, 34): 293,\n",
       " (266, 100): 294,\n",
       " (117, 103): 295,\n",
       " (112, 289): 296,\n",
       " (278, 286): 297,\n",
       " (32, 115): 298,\n",
       " (97, 110): 299}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f87e89cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    \"\"\"\n",
    "    Input: string\n",
    "    Output: tokens\n",
    "    \"\"\"\n",
    "    tokens = text.encode(\"utf-8\", errors=\"replace\")\n",
    "    tokens = list(tokens)\n",
    "    \n",
    "    while len(tokens) >= 2:\n",
    "        freq = get_pair_frequency(tokens)\n",
    "        pair = min(freq, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "\n",
    "        if pair not in merges:\n",
    "            break\n",
    "        \n",
    "        index = merges[pair]\n",
    "        tokens = mint(tokens,pair, index)  \n",
    "    \n",
    "    return tokens    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0d78b699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 101, 108, 108, 111, 285, 266, 108, 100]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "a = encode(\"hello world\")\n",
    "b = decode(a)\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "72e6c695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "text2 = decode(encode(text))\n",
    "print(text == text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "2c05aa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "rand = \"\"\"A common confusion when working with LangGraph is how OverallState and these smaller, node-specific states interact. Let‚Äôs clear that confusion here.\n",
    "\n",
    "The crucial mental model we need to have is this: there is only one state dictionary at runtime, the OverallState.\n",
    "\n",
    "Node-specific TypedDicts are not extra runtime data stores. Instead, they are just typed ‚Äúviews‚Äù onto the one underlying dictionary (OverallState), that temporarily zoom in on the parts a node should see or produce. The purpose of their existence is that the type checker and the LangGraph runtime can enforce clear contracts.\"\"\"\n",
    "\n",
    "rand2 = decode(encode(rand))\n",
    "\n",
    "print(rand == rand2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "8625ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe77369e",
   "metadata": {},
   "source": [
    "- `'s|'t|'re|'ve|'m|'ll|'d` - apostrophes\n",
    "- ` ?\\p{L}+` - optional space followed by any number of letters\n",
    "- ` ?\\p{N}+` - optional space followed by any number of numbers\n",
    "- ` ?[^\\s\\p{L}\\p{N}]+` - optional space followed by anything but letters or numbers\n",
    "- `\\s+(?!\\S)` - spaces not including the last space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "bec639a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ojas', \"'ve\", '025']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(pat, \"ojas've025\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
